#!/usr/bin/env python3

import pyrealsense2 as rs
import rclpy
from rclpy.node import Node
from kobuki_ros_interfaces.msg import *
from std_msgs.msg import String
from sensor_msgs.msg import Image
from tf2_ros import *
from tf2_geometry_msgs import *
import numpy as np
import cv2,time, math
from cv_bridge import CvBridge
import torch
import pathlib

pathlib.WindowsPath = pathlib.PosixPath

def infiniteTalk():
    # Initialize ROS node with ROS client
    rclpy.init()
    aNode = Node('vision')
    control = Vision(aNode)
    # Start infinite loop
    rclpy.spin_once(aNode)
    # Clean everything and switch the light off
    aNode.destroy_node()
    rclpy.shutdown()
    

class Vision:
    def __init__(self, rosNode):

        self.rosNode = rosNode

        self.sound_publisher= rosNode.create_publisher(Sound, '/commands/sound', 10 )
        self.image_publisher = rosNode.create_publisher(Image, '/sensor_msgs/image', 10 )
        self.distance_publisher = rosNode.create_publisher(Pose, 'distance', 10 )
        self._timer = rosNode.create_timer(0.01, self.vision_callback)

        # Configure depth and color streams
        self.pipeline = rs.pipeline()
        self.config = rs.config()
        self.colorizer = rs.colorizer()

        # Create bridge
        self.bridge=CvBridge()

        self.config.enable_stream(rs.stream.color, 848, 480, rs.format.bgr8, 30)
        self.config.enable_stream(rs.stream.depth, 848, 480, rs.format.z16, 30)

        # Start streaming
        self.pipeline.start(self.config)

        self.align_to = rs.stream.color
        self.align = rs.align(self.align_to)

        self.count=1
        self.refTime= time.process_time()
        self.freq= 60

        # yolo model load
        # Chemin absolu vers le modèle et yolov5
        model_path = '/home/o2p9/ros_space/playground/train/ros_project_long/weights/best.pt'
        yolov5_path = '/home/o2p9/yolov5'
        self.model = torch.hub.load(yolov5_path, 'custom', path=model_path, source='local', force_reload=True)

        self.detected = False
        self.previous_detected = False
        self.number_detected = 0
    
    # def __del__(self):
    #     self.pipeline.stop()

    def vision_callback(self):
        
        while True:
            # This call waits until a new coherent set of frames is available on a device
            frames = self.pipeline.wait_for_frames()

            #Aligning color frame to depth frame
            aligned_frames =  self.align.process(frames)
            depth_frame = aligned_frames.get_depth_frame()
            frame = aligned_frames.get_color_frame()

            if not depth_frame or not frame: continue

            # Two ways to colorized the depth map
            # first : using colorizer of pyrealsense                
            colorized_depth = self.colorizer.colorize(depth_frame)
            depth_colormap = np.asanyarray(colorized_depth.get_data())

            # Get the intrinsic parameters
            color_intrin = frame.profile.as_video_stream_profile().intrinsics

            color_image_bgr = np.asanyarray(frame.get_data())
            color_image = cv2.cvtColor(color_image_bgr, cv2.COLOR_BGR2HSV)

            normalized_image = cv2.normalize(color_image, None, 0, 255, cv2.NORM_MINMAX)
            image = cv2.cvtColor(normalized_image, cv2.COLOR_HSV2BGR)

            depth_colormap_dim = depth_colormap.shape
            color_colormap_dim = color_image.shape
            
            results = self.model(color_image_bgr)
            # Récupérer les boîtes de détection, les scores et les classes
            boxes = results.xywh[0]  # coordonnées des boîtes englobantes [x_center, y_center, width, height, confidence, class]
            classes = results.names  # Les classes d'objets détectées
            confidences = boxes[:, 4].tolist()  # Liste des scores de confiance
            labels = boxes[:, 5].tolist()  # Liste des indices des classes détectées
            coords = boxes[:, :4].tolist() 
            
            self.detected = False

            for i in range(len(boxes)):
                x, y, w, h, conf, clc = boxes[i]
                if conf < 0.4:
                    continue
                x = int(x)
                y = int(y)
                w = int(w)
                h = int(h)
                print(x, y, w, h)

                cv2.rectangle(color_image_bgr, (x - w//2, y - h//2), (x + w//2, y + h//2), (0,0,255), 2)
                cv2.circle(color_image_bgr, (int(x), int(y)), 5, (0, 0, 255), 2)


                # Ajouter le texte avec la classe et la confiance sur l'image
                formatted_confidence = f"{conf:.2g}"
                text = f"Conf: {formatted_confidence}"
                font = cv2.FONT_HERSHEY_SIMPLEX
                font_scale = 1
                font_thickness = 1
                text_size = cv2.getTextSize(text, font, font_scale, font_thickness)[0]

                # Calculer la position pour le texte afin qu'il ne dépasse pas de la boîte
                text_x = x - w//2
                text_y = y - h//2  # Juste au-dessus de la boîte

                # Dessiner le texte
                cv2.putText(color_image_bgr, text, (text_x, text_y), font, font_scale, (255, 255, 255), font_thickness)

                if int(x + w/2) < 848 and int(y + h/2) < 480:
                    depth = depth_frame.get_distance(int(x), int(y))
                    dx ,dy, dz = rs.rs2_deproject_pixel_to_point(color_intrin, [x - w//2,y - h//2], depth)
                else:
                    dx, dy, dz = 0, 0, 0

                a = 424/(35*3.14159/180)
                theta = -(int(x) - 424)/a
                distance = math.sqrt(((dx)**2) + ((dy)**2) + ((dz)**2))
                pose = Pose()
                if distance**2<0.04:
                    pose.position.x = (float)(0)
                else:
                    pose.position.x = (float)((distance**2 - 0.04)**0.5)
                pose.position.y = (float)(distance*math.sin(theta))
                pose.position.z = (float)(0.0)
                
                if distance>=0.04:
                    self.detected = True

            if not self.detected:
                self.number_detected = 0
            if self.detected and self.previous_detected:
                self.number_detected += 1
            if self.number_detected == 1:
                sound = Sound()
                sound.value = 0
                self.sound_publisher.publish(sound)
                self.distance_publisher.publish(pose)
            
            # cv2.imshow('camera',color_image)

            msg_image = self.bridge.cv2_to_imgmsg(color_image_bgr,"bgr8")
            msg_image.header.stamp = self.rosNode.get_clock().now().to_msg()
            msg_image.header.frame_id = "image"
            self.image_publisher.publish(msg_image)

            if cv2.waitKey(1)&0xFF==ord('q'):
                break

            # Frequency:
            if self.count == 10 :
                newTime= time.process_time()
                self.freq= 10/((newTime-self.refTime))
                self.refTime= newTime
                self.count= 0
            self.count+= 1

            self.previous_detected = self.detected

def euclidean_distance(a, b):
    return ((a[0]-b[0])**2+(a[1]-b[1])**2)**0.5

# Execute the function.
if __name__ == "__main__":
    infiniteTalk()
